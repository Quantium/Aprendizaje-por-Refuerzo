{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Estrategia Exploración-Explotación_Multi_Armed_Bandit.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quantium/Aprendizaje-por-Refuerzo/blob/main/Notebooks/Estrategia_Exploraci%C3%B3nExplotaci%C3%B3n_Multi_Armed_Bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ADAbH73rfS"
      },
      "source": [
        "#Bibliotecas y Funciones\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ0Ts9FIHEmu"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DHV4hISwZrD_",
        "outputId": "38729841-b24a-4a6c-c5aa-e4e0439e555e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sns' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1594238490.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m#Para as matrices de correlación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mdark_cmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_cmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Select for Dark Mode\n",
        "import seaborn as sns\n",
        "\n",
        "darkmode_selector = True #@param {type:\"boolean\"}\n",
        "if darkmode_selector:\n",
        "  plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "  plt.style.use('dark_background')\n",
        "  plt.rcParams.update({\n",
        "      'axes.edgecolor': 'white',\n",
        "      'axes.labelcolor': 'white',\n",
        "      'text.color': 'white',\n",
        "      'xtick.color': 'white',\n",
        "      'ytick.color': 'white',\n",
        "      'grid.color': 'lightgray',\n",
        "      'grid.linestyle': '--',\n",
        "      'grid.linewidth': 0.5,\n",
        "      'lines.linewidth': 1.5,\n",
        "      'lines.markersize': 5,\n",
        "      'font.size': 10,\n",
        "      'figure.figsize': (8, 6),\n",
        "      'savefig.dpi': 300,\n",
        "      'savefig.bbox': 'tight',\n",
        "      'legend.edgecolor': 'white',\n",
        "      'legend.frameon': True,\n",
        "      'legend.fancybox': False,\n",
        "  })\n",
        "\n",
        "  #Para as matrices de correlación\n",
        "  dark_cmap = sns.color_palette(\"binary\", as_cmap=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTvgN0tvHi3A"
      },
      "source": [
        "def entorno_multi_armed_bandit(maquinas):\n",
        "    '''\n",
        "    Creamos el entorno para el problema \"multi_armed_bandit\" generando aleatoriamente\n",
        "    la distribución de probabilidad de los premios que otorga cada máquina\n",
        "    '''\n",
        "    premio_medio = np.random.uniform(-10, 5, size=maquinas)\n",
        "    desv_estandar = np.random.uniform(0, 5, size=maquinas)\n",
        "    return premio_medio, desv_estandar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srAAbKjhKM_E"
      },
      "source": [
        "def init_Q(maquinas):\n",
        "  '''Inicializa el vector Q en ceros, el vector Q representa el valor esperado\n",
        "   de recompensa de cada máquina'''\n",
        "  Q =np.zeros(shape=(1, maquinas))\n",
        "  return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2yLnkZKLUlp"
      },
      "source": [
        "def selecciona_maquina_exploracion(maquinas):\n",
        "    '''Selecciona una máquina aleatoriamente con distribución unifome'''\n",
        "    selec = np.random.choice(range(maquinas))\n",
        "    return selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selecciona_maquina_epsilon_constante (m  ε)aquinas,:\n",
        "    '''Selecciona la máquina con el máximo valor de la ganancia esperada'''\n",
        "    p = np.random.uniform(0,1)\n",
        "\n",
        "    if p < (1 - ε):\n",
        "        selec = np.argmax(Q[0])\n",
        "        return selec\n",
        "\n",
        "\n",
        "    else:\n",
        "        selec = np.random.choice(range(maquinas))\n",
        "        return selec\n",
        "    return selec"
      ],
      "metadata": {
        "id": "gjjSB5uLS4RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9atc3BHgU2d"
      },
      "source": [
        "def selecciona_maquina_egd(maquinas, ε):\n",
        "    '''Selecciona una máquina con la estrategia epsilon decreasing greedy'''\n",
        "    p = np.random.uniform(0,1)\n",
        "\n",
        "    # cuándo epsilon es pequeño, generalmente se escoge la máquina con mayor ganancia\n",
        "    if p < (1 - ε):\n",
        "        selec = np.argmax(Q[0])\n",
        "        return selec\n",
        "\n",
        "    # cuando epsilon es grande, generalmente se escoge una al azar\n",
        "    else:\n",
        "        selec = np.random.choice(range(maquinas))\n",
        "        return selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUv7oB7VId1t"
      },
      "source": [
        "def calcula_recompensa(selec):\n",
        "  '''Calcula la recompensa de jugar en una determinada máquina'''\n",
        "  r = int(np.random.normal(premio_medio[selec], desv_estandar[selec], 1)[0])\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHfwiInqOkBX"
      },
      "source": [
        "def actualiza_Q (Q, selec, r, veces_maq):\n",
        "    '''Actualiza el valor esperados de recompensa de la máquina seleccionada'''\n",
        "    Q[0, selec] = Q[0, selec] + 1/(veces_maq[selec])*(r - Q[0, selec])\n",
        "    return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzVeNtKjpHrT"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKA6ZoIgr9wq"
      },
      "source": [
        "## Definimos nuestro entorno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FJ8WPRopBiH"
      },
      "source": [
        "### Creamos el entorno definiendo el comportamiento de cada máquina\n",
        "\n",
        "premio_medio  = np.array([1,  2,  3, -10])\n",
        "desv_estandar = np.array([1,  4,   5,  1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maquina = 2\n",
        "n = 10000\n",
        "#  r es el premio aleatorio que va a dar una máquina (dada su distribución)\n",
        "r = np.random.normal(premio_medio[maquina], desv_estandar[maquina],n)\n",
        "plt.hist(r, bins=30, density=True)\n",
        "plt.show()\n",
        "print('\\nLa máquina arrojo durante los primeras 10 jugadas los siguientes premios: \\n')\n",
        "print (r[0:10])"
      ],
      "metadata": {
        "id": "JEnrsD3ZyZI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué hace un jugador en el casino?"
      ],
      "metadata": {
        "id": "b-FpFTSl1UF0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djXUF3Prrubk"
      },
      "source": [
        "* inicializamos en cero la variable que guardará los premios o perdidas acumuladas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Zlrmeum_3q"
      },
      "source": [
        "premio_acumulado = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMOjxNywtLLw"
      },
      "source": [
        "* seleccionamos una de las maquinas,\n",
        "* jugamos en ella y obtenemos la recompensa (o pérdida)\n",
        "* se suma la recompensa o perdida al total acumulado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGjez-pgtLLz"
      },
      "source": [
        "# 1) Se selecciona una máquina.\n",
        "selec = 3\n",
        "\n",
        "# 2) Jugamos en la máquina seleccionada y se obtiene una recompensa o pérdida.\n",
        "r = int(np.random.normal(premio_medio[selec], desv_estandar[selec], 1)[0])\n",
        "\n",
        "# 3) Se suma al premio acumulado.\n",
        "premio_acumulado += r\n",
        "print('recompensa (r):', r)\n",
        "print('Premio acumulado:', premio_acumulado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fOSdzrXvkOE"
      },
      "source": [
        "##  ¿Cómo guardamos la información de los premios obtenidos de cada máquina?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>Promedio Incremental\n",
        "\n",
        "$$Q =  Q_{n-1}+\\frac{1}{n} (r_n - Q_{n-1})$$"
      ],
      "metadata": {
        "id": "HVASmSMgzEe8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnPBUn3qnYn2"
      },
      "source": [
        "# Inicializa el vector Q en ceros, donde guardaremos la información de cada máquina.\n",
        "n_maquinas = 4\n",
        "Q = np.zeros(shape=(1, n_maquinas))\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCwiS-nR0_7D"
      },
      "source": [
        "# Vamos a jugar cierto número de veces (episodios).\n",
        "episodios = 20\n",
        "n_maquinas = 4\n",
        "\n",
        "# Inicializamos el vector Q y la variable \"premio acumulado\".\n",
        "Q = np.zeros(shape =(1, n_maquinas))\n",
        "premio_acumulado = 0\n",
        "\n",
        "# Inicializamos un vector que guardará las veces que se va jugando en cada máquina.\n",
        "\n",
        "veces_maq = np.zeros(n_maquinas)\n",
        "\n",
        "# Creamos un ciclo de juego y aprendizaje.\n",
        "for ep in range(1, episodios+1):\n",
        "      # 1) Se selecciona una máquina, de manera aleatoria, distribución uniforme.\n",
        "      selec = np.random.choice([0, 1, 2, 3])\n",
        "\n",
        "      # 2) Jugamos en la máquina seleccionada y se obtiene una recompensa o pérdida\n",
        "      r = int(np.random.normal(premio_medio[selec], desv_estandar[selec], 1)[0])\n",
        "\n",
        "      # 3) Se suma la recompensa o pérdida obtenida al premio acumulado\n",
        "      premio_acumulado += r\n",
        "\n",
        "      # 5) Actualizamos la información con el premio o castigo obtenido en la máquina seleccionada\n",
        "      veces_maq[selec] += 1 # Actualizamos las veces que se ha jugado en la máquina seleccionada\n",
        "      Q[0, selec] = Q[0, selec] + 1/(veces_maq[selec])*(r - Q[0, selec])\n",
        "\n",
        "      # 6) Reportamos el progreso del aprendizaje.\n",
        "      print(f'Episodio {ep},   Máquina {selec},   Premio = {r},   \\\n",
        "Premio_acumulado = {premio_acumulado},    Q = {Q.round(2)} \\n')\n",
        "\n",
        "print(f'Premio_acumulado = {premio_acumulado},   Q = {Q.round(2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVXPXUUw38V5"
      },
      "source": [
        "¿Sería correcto si, en lugar de escojer la máquina de forma aleatoria, escojemos la que más rendimientos ha dado?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulLG8yCP0xLy"
      },
      "source": [
        "selec = np.argmax(Q[0])  # Selección usando la estrategia de explotación.\n",
        "selec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX6_4S3c8JWN"
      },
      "source": [
        "¿Podemos pensar en una estrategia combinada?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R-1PKtw8lMb"
      },
      "source": [
        "p = np.random.uniform(0, 1)  # Obtenemos un valor entre 0 y 1.\n",
        "ε = .95 # Valor que disminuye desde 1 hasta 0 según avance el aprendizaje.\n",
        "\n",
        "print(f'p = {p}, ε = {ε}')\n",
        "print(f'¿Es p menor que 1 - ε?  {p < (1 - ε)}')\n",
        "if p < (1 - ε):\n",
        "    print('Explotación')\n",
        "    selec = np.argmax(Q[0])\n",
        "else:\n",
        "    # Exploración\n",
        "    print('Exploración')\n",
        "    selec = np.random.choice(range(n_maquinas))\n",
        "print(f'Máquina seleccionada: {selec}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlE8aewIB4mS"
      },
      "source": [
        "p = np.random.uniform(0, 1)  # Obtenemos un valor entre 0 y 1.\n",
        "ε = 0.05 # Valor que disminuye desde 1 hasta 0 según avance el aprendizaje.\n",
        "\n",
        "print(f'p = {p}, ε = {ε}')\n",
        "print(f'¿Es p menor que 1 - ε?  {p < (1 - ε)}')\n",
        "if p < (1 - ε):\n",
        "    print('Explotación')\n",
        "    selec = np.argmax(Q[0])\n",
        "else:\n",
        "    # Exploración\n",
        "    print('Exploración')\n",
        "    selec = np.random.choice(range(n_maquinas))\n",
        "print(f'Máquina seleccionada: {selec}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmve4Vj7CJAf"
      },
      "source": [
        "Para cada episodio ε decairá a una taza de 0.995 a una taza de 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = 1\n",
        "epsilon_decay_rate=0.9925\n",
        "\n",
        "epsilons=[]\n",
        "episodios = 1000\n",
        "for episodio in range(episodios):\n",
        "    e =  e * epsilon_decay_rate\n",
        "    epsilons.append(e)\n",
        "print(e)\n",
        "plt.plot(epsilons)"
      ],
      "metadata": {
        "id": "68-2G5DsRGl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHGYfGfVtZhW"
      },
      "source": [
        "# Aprendizaje con distintas estrategias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QF9KYgctZ9P"
      },
      "source": [
        "# ### Definimos el número de máquinas o brazos del problema\n",
        "# n_maquinas = 3\n",
        "\n",
        "# ### Creamos el entorno\n",
        "# np.random.seed(28)\n",
        "# premio_medio, desv_estandar = entorno_multi_armed_bandit(n_maquinas)  # inicializa la distribución de probabilidad de cada máquina\n",
        "### Definimos el número de episodios (juegos)\n",
        "episodios = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBOk8P63f7Hn"
      },
      "source": [
        "## Exploración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGdvub9qSwmJ"
      },
      "source": [
        "# Inicializamos el vector Q, la variable \"premio acumulado\" y el vector veces_maq.\n",
        "Q = init_Q(n_maquinas)\n",
        "premio_acumulado = 0\n",
        "veces_maq = np.zeros(n_maquinas)\n",
        "\n",
        "# Creamos un ciclo de juego y aprendizaje con estrategia de exploración.\n",
        "for episodio in range(1,episodios+1):\n",
        "    selec = selecciona_maquina_exploracion(n_maquinas)\n",
        "    veces_maq[selec] += 1\n",
        "    r = calcula_recompensa(selec)\n",
        "    Q = actualiza_Q(Q, selec, r, veces_maq)\n",
        "    premio_acumulado += r\n",
        "\n",
        "print(f\"El premio acumulado es de: ${premio_acumulado:,.2f}\".replace('$-', '-$'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "id": "5UnvLZXG82uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDUtGQYnZCL"
      },
      "source": [
        "## Epsilon constante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMTfsHDUnfH4"
      },
      "source": [
        "# Inicializamos el vector Q, la variable \"premio acumulado\" y el vector veces_maq.\n",
        "epsilon = 0.5\n",
        "Q = init_Q(n_maquinas, )\n",
        "premio_acumulado = 0\n",
        "veces_maq = np.zeros(n_maquinas)\n",
        "\n",
        "for episodio in range(episodios+1):\n",
        "    selec = selecciona_maquina_epsilon_constante(n_maquinas, epsilon )\n",
        "    r = calcula_recompensa(selec)\n",
        "    premio_acumulado += r\n",
        "    veces_maq[selec] += 1\n",
        "    Q = actualiza_Q(Q, selec, r, veces_maq)\n",
        "\n",
        "print(f\"El premio acumulado es de: ${premio_acumulado:,.2f}\".replace('$-', '-$'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "id": "vTw3zB6e8xwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXk8RpAf_S5"
      },
      "source": [
        "## Epsilon decreasing greedy (edg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwluzms4f_S7"
      },
      "source": [
        "# Inicializamos el vector Q, la variable \"premio acumulado\" y el vector veces_maq.\n",
        "Q = init_Q(n_maquinas)\n",
        "premio_acumulado = 0\n",
        "veces_maq = np.zeros(n_maquinas)\n",
        "epsilon_decay_rate = 0.9925\n",
        "ε = 1\n",
        "\n",
        "# Creamos un ciclo de juego y aprendizaje con estrategia edg\n",
        "for episodio in range(1 ,episodios+1):\n",
        "    ε =  ε * epsilon_decay_rate\n",
        "    selec = selecciona_maquina_egd(n_maquinas, ε)\n",
        "    r = calcula_recompensa(selec)\n",
        "    premio_acumulado += r\n",
        "    veces_maq[selec] += 1\n",
        "    Q = actualiza_Q(Q, selec, r, veces_maq)\n",
        "\n",
        "print(f\"El premio acumulado es de: ${premio_acumulado:,.2f}\".replace('$-', '-$'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "id": "bkgKQA26TcUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9EM0mu3mB99"
      },
      "source": [
        "# Referencias:\n",
        "\n",
        "[1]A. Aristizabal, «Understanding Reinforcement Learning Hands-On: Multi-Armed Bandits», Medium, oct. 19, 2020. https://towardsdatascience.com/understanding-reinforcement-learning-hands-on-part-2-multi-armed-bandits-526592072bdc (accedido jul. 30, 2021).\n",
        "\n"
      ]
    }
  ]
}